import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# ðŸ“Œ Liste de modÃ¨les disponibles
MODEL_LIST = [
    "all-MiniLM-L6-v2",
    "paraphrase-MiniLM-L3-v2",
    "multi-qa-MiniLM-L6-cos-v1",
    "distiluse-base-multilingual-cased-v2",
]

# ðŸ“‹ Textes et question par dÃ©faut
DEFAULT_TEXTS = [
    "Le soleil brille aujourd'hui sur Paris.",
    "Les voitures Ã©lectriques deviennent populaires.",
    "La cuisine italienne est dÃ©licieuse.",
    "Les pandas vivent principalement en Chine.",
    "L'intelligence artificielle transforme le monde.",
]
DEFAULT_QUESTION = "Quel est l'impact de la technologie sur la sociÃ©tÃ© ?"

# ðŸ§  Interface Streamlit
st.title("ðŸ§¬ Visualisation d'Embeddings avec Sentence Transformers")

model_name = st.selectbox("Choisis un modÃ¨le d'embedding :", MODEL_LIST)
texts = [st.text_input(f"Texte {i + 1}", DEFAULT_TEXTS[i]) for i in range(5)]
question = st.text_input("Question", DEFAULT_QUESTION)
threshold = st.slider("Seuil de similaritÃ© cosinus", 0.0, 1.0, 0.4)


# ðŸ” Chargement du modÃ¨le
@st.cache_resource
def load_model(name):
    return SentenceTransformer(name)


model = load_model(model_name)

# ðŸ”„ Calcul des embeddings
all_sentences = texts + [question]
embeddings = model.encode(all_sentences)

# ðŸ“Š SimilaritÃ© cosinus
similarities = cosine_similarity([embeddings[-1]], embeddings[:-1])[0]

# ðŸ“‰ RÃ©duction dimensionnelle
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# ðŸŽ¯ Affichage des rÃ©sultats
if st.button("GÃ©nÃ©rer le graphe 2D"):
    fig, ax = plt.subplots()
    for i, (text, sim) in enumerate(zip(texts, similarities)):
        if sim >= threshold:
            ax.scatter(*reduced_embeddings[i], label=f"Texte {i + 1} ({sim:.2f})")
            ax.annotate(f"T{i + 1}", reduced_embeddings[i])

    # Position de la question
    ax.scatter(*reduced_embeddings[-1], color="red", label="Question", marker="x")
    ax.annotate("Q", reduced_embeddings[-1])
    ax.set_title("Projection PCA des embeddings")
    ax.legend()
    st.pyplot(fig)
